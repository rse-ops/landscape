<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>Characterizing and Modeling Distributed Training with Transient
               Cloud GPU Servers</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, shrink-to-fit=no">
  <meta http-equiv="content-language" content="en">
  <link href="/landscape/assets/css/style.css" rel="stylesheet">
  <link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:400,700|Roboto+Slab:400,700">
  <link rel="icon" type="image/png" href="/landscape/favicon.ico">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css"/>

</head>
<body>
  <div class="container">
      <h6>Li, Shijian and Walls, Robert J and Guo, Tian</h6>
  <p>Cloud GPU servers have become the de facto way for deep learning
               practitioners to train complex models on large-scale datasets.
               However, it is challenging to determine the appropriate cluster
               configuration-e.g., server type and number-for different
               training workloads while balancing the trade-offs in training
               time, cost, and model accuracy. Adding to the complexity is the
               potential to reduce the monetary cost by using cheaper, but
               revocable, transient GPU servers.In this work, we analyze
               distributed training performance under diverse cluster
               configurations using CM-DARE, a cloud-based measurement and
               training framework. Our empirical datasets include measurements
               from three GPU types, six geographic regions, twenty
               convolutional neural networks, and thousands of Google Cloud
               servers. We also demonstrate the feasibility of predicting
               training speed and overhead using regression-based models.
               Finally, we discuss potential use cases of our performance
               modeling such as detecting and mitigating performance
               bottlenecks.</p>

<div class="language-plaintext highlighter-rouge">
<div class="highlight">
<pre class="highlight"><code>
@inproceedings{Li2020-lm,
  title = {Characterizing and Modeling Distributed Training with Transient
                 Cloud {GPU} Servers},
  booktitle = {2020 {IEEE} 40th International Conference on Distributed
                 Computing Systems ({ICDCS})},
  author = {Li, Shijian and Walls, Robert J and Guo, Tian},
  pages = {943--953},
  month = nov,
  year = {2020},
  keywords = {Training;Computational modeling;Graphics processing
                 units;Predictive models;Internet;Servers;Transient
                 analysis;distributed training;measurement;modeling}
}

</code></pre></div></div>

  <div class="thi-columns">
    <ul class="tag-post">
    
    </ul>
  </div>

  </div>
<br>
<br>
<img id="footer" src="/landscape/assets/img/grass.png">
</div>
</body>
</html>
