Similarly to Continuous integration, continuous deployment (or delivery) is the
effort to distribute a production ready software as often as possible. This
could take the form of tagged version in the code base, source archive, binary,
or container releases. Continuous delivery aims at minimizing the down-time
between a failure of the product, and the release of the fix.

In the DevOps culture, deploying containers is becoming a standard. It allows
for optimal interchangebility, fast roll-backs, and even leads to the trend
to organize the software stack as micro-services.

In HPC the software is configured and built to run on bare metal instead of
relying on generic releases. In practice, we can classify HPC software in two
categories: The cluster software stack is typically installed by the admins,
carefully tuned to acheive stability and performance, with a limited number of
variants of a same product. Scientific code and libraries are most likely
compiled and installed by the final users relying on a configuration often
specific to the simulation code: most installations can be considered unique.

Even in HPC, an approach based on containers could be adopted, however due to
the private nature of scientific software it could be that registries and
services are internal. A self-deployed CI service (e.g., Jenkins or GitLab
\cite{jenkins,gitlab}) that has access to cluster resources could either deploy
an artifact or use SSH to execute a command to do similar. A container could be
pushed to an open source or internal registry.

One important question that results from this is how to control access and
security. If a user builds a container in CI, how is it assessed or scanned to
be safe to deploy?  There is also one additional level of complexity with
providing containers with different architectures to run optimally on the
system. Finally, build caches (meaning binary packages) would be an equally
good solution, or in fact a supplement. Build caches are ideal for reuse, and
containers for reproducibility.
